from datetime import datetime, timedelta
import asyncio
import pendulum

from airflow import DAG
from airflow.operators.python import PythonOperator

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆ ì„í¬íŠ¸
# (Airflowì—ì„œ ê²½ë¡œ ì¸ì‹ì„ ëª»í•˜ë©´ plugins í´ë”ë‚˜ PYTHONPATH ì„¤ì • í•„ìš”í•  ìˆ˜ ìˆìŒ)
from data_pipeline.crawlers.f1_tactic import Formula1Crawler
from data_pipeline.crawlers.f1_news import AutosportCrawler
from data_pipeline.rag_indexer import RAGIndexer
from domain.documents import F1NewsDocument
from motor.motor_asyncio import AsyncIOMotorClient
from beanie import init_beanie

# ---------------------------------------------------------
# 1. ë¹„ë™ê¸° ì‘ì—…ì„ ë™ê¸°ë¡œ ê°ì‹¸ëŠ” ë˜í¼(Wrapper) í•¨ìˆ˜ë“¤
# ---------------------------------------------------------

# DB ì ‘ì† ì •ë³´ (Docker ë‚´ë¶€ í†µì‹ ìš©)
MONGO_URI = "mongodb://mongodb:27017"
QDRANT_URL = "http://qdrant:6333"

# ---------------------------------------------------------
# 1. ë¹„ë™ê¸° ì‘ì—… ì •ì˜ (Crawler Wrappers)
# ---------------------------------------------------------

async def _crawl_and_save_generic(crawler_cls, target_url, platform_name):
    """í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ì™€ íƒ€ê²Ÿ URLì„ ë°›ì•„ì„œ ì‹¤í–‰í•˜ëŠ” ë²”ìš© í•¨ìˆ˜ (ê°œì„ íŒ)"""
    print(f"ğŸï¸ [Task] {platform_name} í¬ë¡¤ë§ ì‹œì‘...")
    
    # DB ì—°ê²°
    client = AsyncIOMotorClient(MONGO_URI)
    await init_beanie(database=client.pitwall_db, document_models=[F1NewsDocument])
    
    crawler = crawler_cls()
    saved_count = 0
    
    try:
        # 1. ëª©ë¡ ìˆ˜ì§‘
        if hasattr(crawler, 'crawl_listing_page'):
            print(f"ğŸ“¡ ëª©ë¡ ìˆ˜ì§‘ ì¤‘... ({target_url})")
            links = crawler.crawl_listing_page(target_url, max_clicks=3) # í´ë¦­ ìˆ˜ ëŠ˜ë¦¼
        else:
            print(f"âš ï¸ {platform_name}: crawl_listing_page ë¯¸êµ¬í˜„. ê±´ë„ˆëœ€.")
            links = []

        print(f"ğŸ“‹ ìˆ˜ì§‘ ëŒ€ìƒ ë§í¬: {len(links)}ê°œ")

        # 2. ê°œë³„ ê¸°ì‚¬ ìˆœíšŒ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
        for i, link in enumerate(links):
            try:
                # ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸ (ë¹„ë™ê¸°)
                exists = await F1NewsDocument.find_one(F1NewsDocument.url == link)
                if exists:
                    # ë„ˆë¬´ ë¡œê·¸ê°€ ë§ìœ¼ë©´ ì‹œë„ëŸ¬ìš°ë‹ˆê¹Œ 10ê°œë§ˆë‹¤ í•˜ë‚˜ì”©ë§Œ ì°ê¸°
                    if i % 10 == 0:
                        print(f"â­ ì¤‘ë³µ ê±´ë„ˆëœ€ ({i}/{len(links)})")
                    continue
                
                # ì¶”ì¶œ (ë™ê¸° í•¨ìˆ˜)
                print(f" [{i+1}/{len(links)}] ì¶”ì¶œ ì‹œë„: {link}")
                data = crawler.extract(link)
                
                if data and data.get('title') and data.get('content'):
                    doc = F1NewsDocument(**data)
                    await doc.insert()
                    saved_count += 1
                    print(f" ì €ì¥ ì™„ë£Œ! (í˜„ì¬ {saved_count}ê±´)")
                else:
                    print(f" ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì €ì¥ ì‹¤íŒ¨: {link}")
                    
            except Exception as inner_e:
                print(f" ê°œë³„ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ({link}): {inner_e}")
                # ì—¬ê¸°ì„œ continueê°€ ë˜ë¯€ë¡œ, í•˜ë‚˜ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ê±° ì§„í–‰í•¨!
                continue

        print(f" {platform_name} ìµœì¢… ì™„ë£Œ. ì´ {saved_count}ê±´ ì‹ ê·œ ì €ì¥.")
        
    except Exception as e:
        print(f" í¬ë¡¤ëŸ¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì—ëŸ¬: {e}")
        raise e # ì´ê±´ Airflowì—ê²Œ ì‹¤íŒ¨ë¥¼ ì•Œë¦¬ê¸° ìœ„í•¨
        
    finally:
        # ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
        if hasattr(crawler, 'driver'):
            crawler.driver.quit()
            print(" ë“œë¼ì´ë²„ ì¢…ë£Œë¨.")

async def _run_rag_indexing():
    print("ğŸ§  [Task] RAG ì¸ë±ì‹± ì‹œì‘")
    indexer = RAGIndexer(mongo_uri=MONGO_URI, qdrant_url=QDRANT_URL)
    await indexer.run_indexing()

# ---------------------------------------------------------
# 2. Airflow Taskìš© ë¸Œë¦¿ì§€ í•¨ìˆ˜
# ---------------------------------------------------------

def task_crawl_f1():
    asyncio.run(_crawl_and_save_generic(
        Formula1Crawler, 
        "https://www.formula1.com/en/latest/tags/analysis.3HkjTN75peeCOsSegCyOWi",
        "Formula1.com"
    ))

def task_crawl_autosport():
    # Autosport F1 ë‰´ìŠ¤ ì„¹ì…˜ URL
    asyncio.run(_crawl_and_save_generic(
        AutosportCrawler, 
        "https://www.autosport.com/f1/news", 
        "Autosport"
    ))

def task_run_indexer():
    asyncio.run(_run_rag_indexing())

# ---------------------------------------------------------
# 3. DAG íŒŒì´í”„ë¼ì¸ ì¡°ë¦½
# ---------------------------------------------------------

default_args = {
    'owner': 'pitwall_engineer',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'pitwall_daily_pipeline',
    default_args=default_args,
    description='Collect F1 News & Indexing',
    schedule_interval=timedelta(days=14), 
    start_date=pendulum.datetime(2024, 1, 1, tz="Asia/Seoul"),
    catchup=False, # ê³¼ê±° ë°ì´í„° ì†Œê¸‰ ì‹¤í–‰ ë°©ì§€
    max_active_runs = 1, # ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” DAG Run ê°¯ìˆ˜ë¥¼ 1ê°œë¡œ ì œí•œ
    tags=['f1', 'rag'],
) as dag:

    # 1. í¬ë¡¤ë§ íƒœìŠ¤í¬ë“¤ (ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥)
    t1_f1 = PythonOperator(
        task_id='crawl_f1_official',
        python_callable=task_crawl_f1
    )

    t2_autosport = PythonOperator(
        task_id='crawl_autosport',
        python_callable=task_crawl_autosport
    )

    # 2. ì¸ë±ì‹± íƒœìŠ¤í¬ (í¬ë¡¤ë§ í›„ ì‹¤í–‰)
    t3_index = PythonOperator(
        task_id='rag_indexing',
        python_callable=task_run_indexer
    )

    # [Dependency Structure]
    # F1í¬ë¡¤ëŸ¬ì™€ Autosportí¬ë¡¤ëŸ¬ëŠ” ë™ì‹œì— ëŒê³ , ë‘˜ ë‹¤ ëë‚˜ë©´ ì¸ë±ì‹± ì‹œì‘
    [t1_f1, t2_autosport] >> t3_index