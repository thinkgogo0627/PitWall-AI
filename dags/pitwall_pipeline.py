from datetime import datetime, timedelta
import asyncio
import pendulum

from airflow import DAG
from airflow.operators.python import PythonOperator

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆ ì„í¬íŠ¸
# (Airflowì—ì„œ ê²½ë¡œ ì¸ì‹ì„ ëª»í•˜ë©´ plugins í´ë”ë‚˜ PYTHONPATH ì„¤ì • í•„ìš”í•  ìˆ˜ ìˆìŒ)
from data_pipeline.crawlers.f1_tactic import Formula1Crawler
from data_pipeline.crawlers.f1_news import AutosportCrawler
from data_pipeline.rag_indexer import RAGIndexer
from domain.documents import F1NewsDocument
from motor.motor_asyncio import AsyncIOMotorClient
from beanie import init_beanie

# ---------------------------------------------------------
# 1. ë¹„ë™ê¸° ì‘ì—…ì„ ë™ê¸°ë¡œ ê°ì‹¸ëŠ” ë˜í¼(Wrapper) í•¨ìˆ˜ë“¤
# ---------------------------------------------------------

# DB ì ‘ì† ì •ë³´ (Docker ë‚´ë¶€ í†µì‹ ìš©)
MONGO_URI = "mongodb://mongodb:27017"
QDRANT_URL = "http://qdrant:6333"

# ---------------------------------------------------------
# 1. ë¹„ë™ê¸° ì‘ì—… ì •ì˜ (Crawler Wrappers)
# ---------------------------------------------------------

async def _crawl_and_save_generic(crawler_cls, target_url, platform_name):
    """í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ì™€ íƒ€ê²Ÿ URLì„ ë°›ì•„ì„œ ì‹¤í–‰í•˜ëŠ” ë²”ìš© í•¨ìˆ˜"""
    print(f"ğŸï¸ [Task] {platform_name} í¬ë¡¤ë§ ì‹œì‘...")
    
    client = AsyncIOMotorClient(MONGO_URI)
    await init_beanie(database=client.pitwall_db, document_models=[F1NewsDocument])
    
    crawler = crawler_cls()
    
    # ëª©ë¡ ìˆ˜ì§‘ (AutosportëŠ” ë°©ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜, ì—¬ê¸°ì„  ì¸í„°í˜ì´ìŠ¤ê°€ ê°™ë‹¤ê³  ê°€ì •)
    # ë§Œì•½ AutosportCrawlerì— crawl_listing_pageê°€ ì—†ë‹¤ë©´ êµ¬í˜„ í•„ìš”
    # (ìš°ë¦¬ê°€ ë§Œë“  AutosportCrawlerëŠ” í˜„ì¬ ë‹¨ì¼ ë§í¬ extractë§Œ êµ¬í˜„ë˜ì–´ ìˆìŒ -> TODO ì²´í¬ í•„ìš”)
    # ì¼ë‹¨ ë‹¨ì¼ ë§í¬ í…ŒìŠ¤íŠ¸ìš© ë¡œì§ìœ¼ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ë¡œì§ ì¶”ê°€ í•„ìš”
    
    # [ì£¼ì˜] AutosportCrawlerì—ë„ crawl_listing_page ë©”ì„œë“œë¥¼ Formula1Crawlerì²˜ëŸ¼ ì¶”ê°€í•´ì•¼ í•¨
    # í˜„ì¬ëŠ” ì˜ˆì‹œë¡œ Autosport ë©”ì¸ ë‰´ìŠ¤ í˜ì´ì§€ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•¨
    try:
        if hasattr(crawler, 'crawl_listing_page'):
            links = crawler.crawl_listing_page(target_url, max_clicks=1)
        else:
            # ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ê¸°ëŠ¥ì´ ì—†ìœ¼ë©´ ì„ì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ (êµ¬í˜„ í•„ìš” ì•Œë¦¼)
            print(f"âš ï¸ {platform_name}: crawl_listing_page ë©”ì„œë“œ ë¯¸êµ¬í˜„ ìƒíƒœ")
            links = []

        saved_count = 0
        for link in links:
            exists = await F1NewsDocument.find_one(F1NewsDocument.url == link)
            if exists:
                continue
            
            data = crawler.extract(link)
            if data and data.get('title'):
                doc = F1NewsDocument(**data)
                await doc.insert()
                saved_count += 1
                
        print(f"ğŸ {platform_name} ì™„ë£Œ. {saved_count}ê±´ ì €ì¥.")
    finally:
        crawler.driver.quit()

async def _run_rag_indexing():
    print("ğŸ§  [Task] RAG ì¸ë±ì‹± ì‹œì‘")
    indexer = RAGIndexer(mongo_uri=MONGO_URI, qdrant_url=QDRANT_URL)
    await indexer.run_indexing()

# ---------------------------------------------------------
# 2. Airflow Taskìš© ë¸Œë¦¿ì§€ í•¨ìˆ˜
# ---------------------------------------------------------

def task_crawl_f1():
    asyncio.run(_crawl_and_save_generic(
        Formula1Crawler, 
        "https://www.formula1.com/en/latest/tags/analysis.3HkjTN75peeCOsSegCyOWi",
        "Formula1.com"
    ))

def task_crawl_autosport():
    # Autosport F1 ë‰´ìŠ¤ ì„¹ì…˜ URL
    asyncio.run(_crawl_and_save_generic(
        AutosportCrawler, 
        "https://www.autosport.com/f1/news", 
        "Autosport"
    ))

def task_run_indexer():
    asyncio.run(_run_rag_indexing())

# ---------------------------------------------------------
# 3. DAG íŒŒì´í”„ë¼ì¸ ì¡°ë¦½
# ---------------------------------------------------------

default_args = {
    'owner': 'pitwall_engineer',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'pitwall_daily_pipeline',
    default_args=default_args,
    description='Collect F1 News & Indexing',
    schedule_interval=timedelta(days=14), 
    start_date=pendulum.datetime(2024, 1, 1, tz="Asia/Seoul"),
    catchup=False, # ê³¼ê±° ë°ì´í„° ì†Œê¸‰ ì‹¤í–‰ ë°©ì§€
    tags=['f1', 'rag'],
) as dag:

    # 1. í¬ë¡¤ë§ íƒœìŠ¤í¬ë“¤ (ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥)
    t1_f1 = PythonOperator(
        task_id='crawl_f1_official',
        python_callable=task_crawl_f1
    )

    t2_autosport = PythonOperator(
        task_id='crawl_autosport',
        python_callable=task_crawl_autosport
    )

    # 2. ì¸ë±ì‹± íƒœìŠ¤í¬ (í¬ë¡¤ë§ í›„ ì‹¤í–‰)
    t3_index = PythonOperator(
        task_id='rag_indexing',
        python_callable=task_run_indexer
    )

    # [Dependency Structure]
    # F1í¬ë¡¤ëŸ¬ì™€ Autosportí¬ë¡¤ëŸ¬ëŠ” ë™ì‹œì— ëŒê³ , ë‘˜ ë‹¤ ëë‚˜ë©´ ì¸ë±ì‹± ì‹œì‘
    [t1_f1, t2_autosport] >> t3_index