import asyncio
from motor.motor_asyncio import AsyncIOMotorClient
from beanie import init_beanie

# ë„ë©”ì¸ ëª¨ë¸ & í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
from domain.documents import F1NewsDocument
from data_pipeline.crawlers.f1_tactic import Formula1Crawler


async def test_crawler_logic():
    print("ğŸ”Œ MongoDB ì‹œë™(Fuel Injection) ì¤‘...")
    
    # ---------------------------------------------------------
    # 1. í•„ìˆ˜: DB ì—°ê²° (ì´ê²Œ ìˆì–´ì•¼ Documentë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ!)
    # ---------------------------------------------------------
    mongo_uri = "mongodb://admin:password123@localhost:27017"
    try:
        client = AsyncIOMotorClient(mongo_uri)
        # pitwall_dbì— F1NewsDocument ë“±ë¡
        await init_beanie(database=client.pitwall_db, document_models=[F1NewsDocument])
        print("âœ… DB ì—°ê²° ì„±ê³µ! (Ready to Race)")
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # ---------------------------------------------------------
    # 2. í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘
    # ---------------------------------------------------------
    crawler = Formula1Crawler()
    
    # ê¸°ì‚¬ ëª©ë¡ í˜ì´ì§€ (Tactic/Analysis íƒœê·¸)
    target_list_url = "https://www.formula1.com/en/latest/tags/analysis.3HkjTN75peeCOsSegCyOWi"
    
    # (1) ëª©ë¡ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    print(f"\nğŸš€ [TEST] ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ (íƒ€ê²Ÿ: {target_list_url})")
    # í…ŒìŠ¤íŠ¸ë‹ˆê¹Œ 1~2ë²ˆë§Œ í´ë¦­í•´ì„œ ë¹ ë¥´ê²Œ í™•ì¸
    article_links = crawler.crawl_listing_page(target_list_url, max_clicks=2)
    
    print(f"ğŸ“¦ ì´ {len(article_links)}ê°œì˜ ë§í¬ í™•ë³´!")
    if not article_links:
        print("âŒ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨")
        return

    # (2) ê°œë³„ ê¸°ì‚¬ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ë§í¬ë¡œ)
    target_article = article_links[0]
    print(f"\nğŸš€ [TEST] ê°œë³„ ê¸°ì‚¬ ìƒì„¸ ìˆ˜ì§‘: {target_article}")
    
    # ì´ì œ DBê°€ ì—°ê²°ë˜ì–´ ìˆìœ¼ë‹ˆ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ì•ˆ ë‚¨!
    result_dict = crawler.extract(target_article)
    
    if result_dict and result_dict.get('title'):
        # [NEW] DBì— ì§„ì§œë¡œ ì €ì¥í•˜ëŠ” ì½”ë“œ ì¶”ê°€!
        # 1. ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜
        doc = F1NewsDocument(**result_dict)
        
        # 2. ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€) í›„ ì €ì¥
        existing_doc = await F1NewsDocument.find_one(F1NewsDocument.url == doc.url)
        if not existing_doc:
            await doc.insert()
            print(f"ğŸ’¾ [ì €ì¥ ì™„ë£Œ] MongoDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ID: {doc.id})")
        else:
            print(f"âš ï¸ [ì¤‘ë³µ] ì´ë¯¸ DBì— ìˆëŠ” ê¸°ì‚¬ì…ë‹ˆë‹¤.")
            
        print(f" - ì œëª©: {doc.title}")
    else:
        print("âŒ ì‹¤íŒ¨: ë‚´ìš©ì„ ê°€ì ¸ì˜¤ì§€ ëª»í•¨")

    # ë¸Œë¼ìš°ì € ì¢…ë£Œ
    crawler.driver.quit()

if __name__ == "__main__":
    asyncio.run(test_crawler_logic())