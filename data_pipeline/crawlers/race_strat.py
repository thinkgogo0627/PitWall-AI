from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import time
import random

# ì „ì—­ ìƒìˆ˜
BASE_URL = "https://www.formula1.com/en/latest/tags/analysis.3HkjTN75peeCOsSegCyOWi"

def setup_driver():
    """WSL í™˜ê²½ì— ìµœì í™”ëœ í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •"""
    chrome_options = Options()
    chrome_options.add_argument("--headless") # WSL í•„ìˆ˜
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def get_article_links(driver, limit=5):
    """ë¶„ì„(Analysis) ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URL ìˆ˜ì§‘"""
    print(f"ğŸŒ F1 ê³µí™ˆ ì§„ì…: {BASE_URL}")
    driver.get(BASE_URL)
    time.sleep(3) # ë¡œë”© ëŒ€ê¸°

    # ì¿ í‚¤ íŒì—… ë‹«ê¸° ì‹œë„
    try:
        cookie_btn = driver.find_element(By.ID, "sp-cc-accept")
        cookie_btn.click()
    except:
        pass

    links = driver.find_elements(By.TAG_NAME, "a")
    target_links = []
    seen_urls = set()

    print(f"ğŸ” ë§í¬ ìŠ¤ìº” ì¤‘...")
    
    for link in links:
        try:
            href = link.get_attribute('href')
            title = link.text.strip()
            
            # í•„í„°ë§ ì¡°ê±´
            if href and '/en/latest/article' in href and title:
                # ì˜ìƒì´ë‚˜ íŒŸìºìŠ¤íŠ¸ ì œì™¸
                if "Video" not in title and "Podcast" not in title:
                    if href not in seen_urls:
                        seen_urls.add(href)
                        target_links.append({"href": href, "title": title})
                        print(f"  ğŸ¯ [Target] {title[:40]}...")
                        
            if len(target_links) >= limit:
                break
        except:
            continue
            
    return target_links

def extract_content(driver, url):
    """ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        driver.get(url)
        time.sleep(random.uniform(1.5, 3.0))
        
        paragraphs = driver.find_elements(By.TAG_NAME, "p")
        content = []
        
        for p in paragraphs:
            text = p.text.strip()
            # ë…¸ì´ì¦ˆ ì œê±°
            if len(text) > 40 and "cookie" not in text.lower() and "Â©" not in text:
                content.append(text)
        
        return " ".join(content)
    except Exception as e:
        print(f"    âŒ ë³¸ë¬¸ ì¶”ì¶œ ì—ëŸ¬: {e}")
        return ""

def crawl(limit=5):
    """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    driver = None
    articles = []
    
    try:
        driver = setup_driver()
        print("ğŸšœ F1 ê³µì‹ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ì‹œì‘...")
        
        # 1. ë§í¬ ìˆ˜ì§‘
        targets = get_article_links(driver, limit)
        print(f"ì´ {len(targets)}ê°œì˜ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 2. ë³¸ë¬¸ ìˆœíšŒ
        for item in targets:
            print(f"  ğŸ‘‰ ì ‘ì†: {item['title'][:30]}...")
            text = extract_content(driver, item['href'])
            
            if len(text) > 500:
                articles.append({
                    "title": item['title'],
                    "link": item['href'],
                    "context": text,
                    "source": "F1 Official Analysis"
                })
                print(f"    âœ… ì„±ê³µ ({len(text)}ì)")
            else:
                print("    âš ï¸ ì‹¤íŒ¨ (ë‚´ìš© ë¶€ì¡±)")
                
    except Exception as e:
        print(f"ğŸ”¥ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        
    finally:
        if driver:
            driver.quit()
            print("ğŸšœ ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ.")
            
    return pd.DataFrame(articles)