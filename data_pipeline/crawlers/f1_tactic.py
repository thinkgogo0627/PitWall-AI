import traceback
import trafilatura
from bs4 import BeautifulSoup
from datetime import datetime
import time

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from .base import BaseSeleniumCrawler
from domain.documents import F1NewsDocument

class Formula1Crawler(BaseSeleniumCrawler):
    
    def set_extra_driver_options(self, options) -> None:
        # [ìŠ¤í…”ìŠ¤ ëª¨ë“œ] F1 ê³µì‹ í™ˆí˜ì´ì§€ë„ ë´‡ íƒì§€ê°€ ìˆìœ¼ë¯€ë¡œ ìœ„ì¥ìˆ  ì ìš©
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        # [ì¤‘ìš”] Eager ì „ëµ: ì´ë¯¸ì§€ê°€ ë‹¤ ëœ¨ê¸° ì „ì— í…ìŠ¤íŠ¸ë§Œ ë‚šì•„ì±” (ì†ë„ í–¥ìƒ)
        options.page_load_strategy = 'eager'

        # ìœ ì € ì—ì´ì „íŠ¸: ì¼ë°˜ ìœˆë„ìš° í¬ë¡¬ì¸ ì²™
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
    def crawl_listing_page(self, list_url: str, max_clicks: int = 3) -> list:
        print(f" ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ (Max Clicks: {max_clicks}): {list_url}")
        links = []
        
        try:
            self.driver.get(list_url)
            time.sleep(5)

            # 1. ì¿ í‚¤ ë°°ë„ˆ ì²˜ë¦¬ (ê°€ë” ë²„íŠ¼ì„ ê°€ë¦¼)
            try:
                cookie_btn = self.driver.find_element(By.ID, "truste-consent-button")
                cookie_btn.click()
                print(" ì¿ í‚¤ ë°°ë„ˆ ì œê±°ë¨")
                time.sleep(1)
            except:
                pass

            # 2. Load More ë²„íŠ¼ ì—°íƒ€ ë£¨í”„
            click_count = 0
            while click_count < max_clicks:
                try:
                    # ìŠ¤í¬ë¡¤ ìµœí•˜ë‹¨ ì´ë™
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)

                    # ë²„íŠ¼ ì°¾ê¸° (More News, Load more ë“± í…ìŠ¤íŠ¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ìœ ì—°í•˜ê²Œ)
                    # XPath: 'Load more'ë¼ëŠ” í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ë²„íŠ¼ì´ë‚˜ aíƒœê·¸ ê²€ìƒ‰
                    btn_xpath = "//button[contains(., 'Load more')] | //a[contains(., 'Load more')]"
                    
                    load_more_btn = WebDriverWait(self.driver, 5).until(
                        EC.element_to_be_clickable((By.XPATH, btn_xpath))
                    )
                    
                    # JSë¡œ ê°•ì œ í´ë¦­ (í™”ë©´ ê°€ë¦¼ ë°©ì§€)
                    self.driver.execute_script("arguments[0].click();", load_more_btn)
                    print(f"ğŸ–±ï¸ Load More í´ë¦­ ì„±ê³µ ({click_count+1}/{max_clicks})")
                    
                    time.sleep(3) # ìƒˆ í•­ëª© ë¡œë”© ëŒ€ê¸°
                    click_count += 1
                
                except Exception:
                    print(" ë” ì´ìƒ Load More ë²„íŠ¼ì´ ì—†ìŒ (ë˜ëŠ” ë ë„ë‹¬)")
                    break

            # 3. ì „ì²´ HTML íŒŒì‹± ë° ë§í¬ ì¶”ì¶œ
            print(" ë§í¬ ì¶”ì¶œ ì¤‘...")
            soup = BeautifulSoup(self.driver.page_source, "html.parser")

            # [ë””ë²„ê¹…ìš©] a íƒœê·¸ê°€ ì•„ì˜ˆ ì•ˆ ì¡íˆëŠ”ì§€ í™•ì¸
            all_links = soup.find_all("a", href=True)
            print(f"DEBUG: í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ì „ì²´ ë§í¬ ìˆ˜: {len(all_links)}ê°œ")
            
            for a in all_links:
                href = a['href']
                # /article/ íŒ¨í„´ì´ ìˆê³ , .htmlë¡œ ëë‚˜ëŠ” ê²ƒë§Œ (ë™ì˜ìƒ ë“± ì œì™¸)
                if "/article/" in href and "video" not in href:
                    full_link = "https://www.formula1.com" + href if href.startswith("/") else href
                    links.append(full_link)
                    # ë””ë²„ê¹… -> ì²˜ìŒ 3ê°œë§Œ ì–´ë–¤ ëª¨ì–‘ì¸ì§€ ì²´í¬
                    if len(links) <= 3:
                        print(f"    -> í™•ë³´ëœ í›„ë³´: {full_link}")

            links = list(set(links)) # ì¤‘ë³µ ì œê±°
            print(f" ì´ {len(links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ í™•ë³´!")
            
        except Exception as e:
            print(f" ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            print(traceback.format_exc())
            
        return links
      

    # one ê¸°ì‚¬ì—ì„œ schemaì— ë§ì¶° ë°ì´í„° ë½‘ì•„ì˜¤ëŠ” ë©”ì„œë“œ
    def extract(self, link: str, **kwargs) -> dict:
        print(f"ğŸï¸ F1.com ì§„ì… ì¤‘: {link}")
        
        try:
            self.driver.get(link)
            # Eager ëª¨ë“œë¼ ë¼ˆëŒ€ë§Œ ë¡œë”©ë˜ë¯€ë¡œ, ë³¸ë¬¸ì´ ë Œë”ë§ë  ì§¬ì„ ì‚´ì§ ì¤Œ (1~2ì´ˆ)
            time.sleep(3) 
            
            html_source = self.driver.page_source

            # [Step 1] ì œëª© ì¶”ì¶œ (BeautifulSoup)
            soup = BeautifulSoup(html_source, "html.parser")
            
            # F1.comì€ ë³´í†µ h1 íƒœê·¸ì— 'f1-header__title' ê°™ì€ í´ë˜ìŠ¤ê°€ ë¶™ì§€ë§Œ, 
            # ë²”ìš©ì„±ì„ ìœ„í•´ h1 ìš°ì„  ê²€ìƒ‰
            title_tag = soup.find("h1")
            title = title_tag.get_text(strip=True) if title_tag else "No Title"

            # [Step 2] ë³¸ë¬¸ ì¶”ì¶œ (Trafilatura ì—”ì§„) 
            # ê³µì‹ í™ˆì€ 'Video'ë‚˜ 'Related' ìœ„ì ¯ì´ ë§ì•„ì„œ Trafilaturaê°€ ì•„ì£¼ íš¨ê³¼ì ì„
            body_content = trafilatura.extract(
                html_source, 
                include_comments=False, 
                include_tables=False, 
                no_fallback=False
            )

            if not body_content:
                print(f"âš ï¸ Trafilatura ì¶”ì¶œ ì‹¤íŒ¨ -> HTML êµ¬ì¡° í™•ì¸ í•„ìš”")
                body_content = "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

            # [Step 3] ì‘ì„±ì (ì„ íƒ ì‚¬í•­)
            # F1.comì€ ì‘ì„±ìê°€ ì—†ê±°ë‚˜ 'Formula 1'ìœ¼ë¡œ í‘œê¸°ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
            author = "Formula 1 Official" 

            # [Step 4] ë¬¸ì„œ ìƒì„±
            news_doc = F1NewsDocument(
                title=title,
                content=body_content,
                url=link,
                platform="Formula1.com", # í”Œë«í¼ ëª…ì‹œ
                author=author,
                published_at=datetime.now(),
                is_embedded=False
            )
            
            print(f" ì¶”ì¶œ ì™„ë£Œ: {title} ({len(body_content)}ì)")
            return news_doc.dict()

        except Exception as e:
            print(f" F1.com í¬ë¡¤ë§ ì‹¤íŒ¨")
            print(traceback.format_exc())
            return {}