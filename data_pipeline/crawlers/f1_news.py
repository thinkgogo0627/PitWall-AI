# data_pipeline/crawlers/f1_news.py

import traceback
import trafilatura
import time
from bs4 import BeautifulSoup
from datetime import datetime

# [NEW] ëª©ë¡ ìˆ˜ì§‘ì„ ìœ„í•œ Selenium í•„ìˆ˜ ë„êµ¬ë“¤ ì¶”ê°€
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from .base import BaseSeleniumCrawler
from domain.documents import F1NewsDocument

class AutosportCrawler(BaseSeleniumCrawler):
    
    def set_extra_driver_options(self, options) -> None:
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        options.page_load_strategy = 'eager'

    # -------------------------------------------------------------------------
    # [NEW] ëª©ë¡ í˜ì´ì§€ ìˆœíšŒ ë° ë§í¬ ìˆ˜ì§‘ (Autosport ì „ìš©)
    # -------------------------------------------------------------------------
    def crawl_listing_page(self, list_url: str, max_clicks: int = 3) -> list:
        print(f"ğŸ“¡ [Autosport] ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘: {list_url}")
        links = []
        
        try:
            self.driver.get(list_url)
            time.sleep(3) # ì²« ì§„ì… ëŒ€ê¸°

            # 1. ì¿ í‚¤ ë°°ë„ˆ ì²˜ë¦¬ (ìœ ëŸ½ ì‚¬ì´íŠ¸ë¼ í•„ìˆ˜)
            try:
                # AutosportëŠ” ë³´í†µ 'Accept All' í˜¹ì€ 'Agree' ë²„íŠ¼ì´ ìˆìŒ
                # IDë‚˜ Classê°€ ìì£¼ ë°”ë€Œë¯€ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì°¾ìŒ
                cookie_xpath = "//button[contains(., 'Accept')] | //button[contains(., 'Agree')]"
                cookie_btn = self.driver.find_element(By.XPATH, cookie_xpath)
                cookie_btn.click()
                print("ğŸª ì¿ í‚¤ ë°°ë„ˆ ì œê±°ë¨")
                time.sleep(1)
            except:
                pass # ë°°ë„ˆê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•´ë„ ì¼ë‹¨ ì§„í–‰

            # 2. Load More ë²„íŠ¼ í´ë¦­ ë£¨í”„
            click_count = 0
            while click_count < max_clicks:
                try:
                    # ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ë²„íŠ¼ì„ ë…¸ì¶œì‹œí‚´
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1)
                    
                    # Autosportì˜ 'Load more' ë²„íŠ¼ ì°¾ê¸°
                    # (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì„ í”¼í•˜ê¸° ìœ„í•´ translate ì‚¬ìš©í•˜ê±°ë‚˜ ì—¬ëŸ¬ ì¡°ê±´ ê²€ìƒ‰)
                    btn_xpath = "//a[contains(translate(., 'L', 'l'), 'load more')] | //button[contains(translate(., 'L', 'l'), 'load more')]"
                    
                    load_more_btn = WebDriverWait(self.driver, 5).until(
                        EC.element_to_be_clickable((By.XPATH, btn_xpath))
                    )
                    
                    self.driver.execute_script("arguments[0].click();", load_more_btn)
                    print(f"ğŸ–±ï¸ Load More í´ë¦­ ì„±ê³µ ({click_count+1}/{max_clicks})")
                    
                    time.sleep(3) # ë¡œë”© ëŒ€ê¸°
                    click_count += 1
                except Exception:
                    print("ğŸ›‘ ë” ì´ìƒ Load More ë²„íŠ¼ì´ ì—†ìŠµë‹ˆë‹¤. (Loop ì¢…ë£Œ)")
                    break
            
            # 3. HTML íŒŒì‹± ë° ë§í¬ ì¶”ì¶œ
            print("ğŸ“¥ ë§í¬ ì¶”ì¶œ ì¤‘...")
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            
            # Autosport ë‰´ìŠ¤ ë§í¬ íŒ¨í„´: ë³´í†µ /f1/news/... í˜•ì‹ì„ ë”°ë¦„
            all_links = soup.find_all("a", href=True)
            
            for a in all_links:
                href = a['href']
                # ì¡°ê±´: '/f1/news/' ê°€ í¬í•¨ë˜ì–´ ìˆê³ , ëŒ“ê¸€ ì•µì»¤(#)ë‚˜ ë¹„ë””ì˜¤ ì œì™¸
                if "/f1/news/" in href and "#" not in href:
                    # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬ (/f1/news/...)
                    if href.startswith("/"):
                        full_link = "https://www.autosport.com" + href
                    else:
                        full_link = href
                        
                    links.append(full_link)

            links = list(set(links)) # ì¤‘ë³µ ì œê±°
            print(f"ğŸ“¦ [Autosport] ì´ {len(links)}ê°œì˜ ë§í¬ í™•ë³´ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            print(traceback.format_exc())
            
        return links

    # -------------------------------------------------------------------------
    # [EXISTING] ê°œë³„ ê¸°ì‚¬ ì¶”ì¶œ
    # -------------------------------------------------------------------------
    def extract(self, link: str, **kwargs) -> dict:
        print(f"ğŸï¸ Autosport ì§„ì… ì¤‘: {link}")
        
        try:
            self.driver.get(link)
            time.sleep(1) 
            
            html_source = self.driver.page_source

            # [Step 1] ì œëª© ì¶”ì¶œ
            soup = BeautifulSoup(html_source, "html.parser")
            title_tag = soup.find("h1")
            title = title_tag.get_text(strip=True) if title_tag else "No Title"

            # [Step 2] ë³¸ë¬¸ ì¶”ì¶œ
            body_content = trafilatura.extract(
                html_source, 
                include_comments=False, 
                include_tables=False,
                no_fallback=False 
            )

            if not body_content:
                description = soup.find("meta", attrs={"name": "description"})
                body_content = description["content"] if description else "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
                print(f"âš ï¸ Trafilatura ì¶”ì¶œ ì‹¤íŒ¨ -> Meta Descriptionìœ¼ë¡œ ëŒ€ì²´")

            # [Step 3] ì‘ì„±ì ì¶”ì¶œ
            author_tag = soup.find("a", class_="ms-item_author")
            author = author_tag.get_text(strip=True) if author_tag else "Autosport Staff"

            news_doc = F1NewsDocument(
                title=title,
                content=body_content,
                url=link,
                platform="Autosport",
                author=author,
                published_at=datetime.now(),
                is_embedded=False
            )
            
            print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {title} ({len(body_content)}ì)")
            return news_doc.dict()

        except Exception as e:
            print(f"âŒ Autosport í¬ë¡¤ë§ ì‹¤íŒ¨: {link}")
            print(traceback.format_exc())
            return {}