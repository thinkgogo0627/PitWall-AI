import io
import re
import requests
import traceback
import os
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from datetime import datetime
from pypdf import PdfReader
from domain.documents import F1NewsDocument

class FIARegulationCrawler:
    def __init__(self):
        # 2026ë…„ ê·œì • (Issue 8, Issue 2 ë“± ìµœì‹  ë²„ì „ ë°˜ì˜)
        self.urls = {
            "2026_technical": "https://www.fia.com/sites/default/files/fia_2026_formula_1_technical_regulations_issue_8_-_2024-06-24.pdf",
            "2026_sporting": "https://api.fia.com/sites/default/files/fia_2026_f1_regulations_-_section_b_sporting_-_iss02_-_2024-12-11.pdf",
            # í•„ìš”í•œ ê²½ìš° 2025ë…„ë„ ì¶”ê°€ ê°€ëŠ¥
            "2025_sporting": "https://www.fia.com/sites/default/files/fia_2025_formula_1_sporting_regulations_-_issue_1_-_2024-07-31.pdf",
            "2025_technical": "https://www.fia.com/sites/default/files/fia_2025_formula_1_technical_regulations_-_issue_01_-_2024-12-11_1.pdf"
        }

    def _clean_text(self, text: str) -> str:
        """
        PDF ë…¸ì´ì¦ˆ ì œê±°: 
        1. í˜ì´ì§€ ë²ˆí˜¸ (Page X of Y)
        2. ìƒë‹¨ í—¤ë” (Issue X, Date ë“±)
        3. ë¶ˆí•„ìš”í•œ ê³µë°±
        """
        if not text: return ""

        # 1. í—¤ë”/í‘¸í„° íŒ¨í„´ ì œê±° (ì˜ˆ: "Issue 8", "24 June 2024", "Page 1/100")
        # (ë‹¨ìˆœí•œ íŒ¨í„´ ë§¤ì¹­ì´ë¯€ë¡œ ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì„)
        text = re.sub(r'Page \d+/\d+', '', text) 
        text = re.sub(r'\d{1,2} [A-Za-z]+ \d{4}', '', text) # ë‚ ì§œ í˜•ì‹ ì œê±°
        text = re.sub(r'Issue \d+', '', text)

        # 2. ë‹¤ì¤‘ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def _extract_articles(self, text: str, page_num: int, doc_type: str) -> list:
        """
        (Advanced) í˜ì´ì§€ ë‚´ì—ì„œ 'Article X.Y' íŒ¨í„´ì„ ì°¾ì•„ì„œ 
        ë‹¨ìˆœ í˜ì´ì§€ ë²ˆí˜¸ë³´ë‹¤ ë” ì˜ë¯¸ ìˆëŠ” ì œëª©ì„ ìƒì„±í•˜ë ¤ ë…¸ë ¥í•¨.
        """
        # ì •ê·œì‹ìœ¼ë¡œ 'ARTICLE 1.2' ê°™ì€ íŒ¨í„´ ì°¾ê¸°
        article_matches = re.findall(r'ARTICLE\s+(\d+(\.\d+)*)', text, re.IGNORECASE)
        
        chunks = []
        
        # ë§Œì•½ í˜ì´ì§€ ì•ˆì— ëª…í™•í•œ Article ì‹œì‘ì ì´ ìˆë‹¤ë©´ ì œëª©ì— í¬í•¨
        if article_matches:
            # ê°€ì¥ ì²˜ìŒ ë°œê²¬ëœ Article ë²ˆí˜¸ë¥¼ ëŒ€í‘œ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
            main_article = article_matches[0][0]
            title = f"FIA {doc_type} Regulations - Art {main_article} (Page {page_num})"
        else:
            title = f"FIA {doc_type} Regulations - Page {page_num}"

        # ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë§ì¶”ê¸° (F1NewsDocument ì¬í™œìš©)
        # ê·œì •ì§‘ì€ 'ë‰´ìŠ¤'ëŠ” ì•„ë‹ˆì§€ë§Œ, RAGì—ì„œëŠ” ê°™ì€ ë¬¸ì„œë¡œ ì·¨ê¸‰í•˜ëŠ” ê²Œ ê´€ë¦¬í•˜ê¸° í¸í•¨
        chunks.append({
            "title": title,
            "content": text,
            "page_no": page_num
        })
        
        return chunks

    def crawl(self, doc_key: str) -> list:
        """
        ì‹¤ì œ í¬ë¡¤ë§ ë° íŒŒì‹± ì‹¤í–‰
        doc_key: '2026_technical', '2026_sporting' ë“±
        """
        target_url = self.urls.get(doc_key)
        if not target_url:
            print(f" ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ í‚¤ì…ë‹ˆë‹¤: {doc_key}")
            return []

        print(f"ğŸ“¥ FIA ê·œì •ì§‘ ë‹¤ìš´ë¡œë“œ ì¤‘... [{doc_key}]")
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(target_url, headers=headers, timeout=60)
            
            if response.status_code != 200:
                print(f" ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: Status {response.status_code}")
                return []
            
            # PDF ë©”ëª¨ë¦¬ ë¡œë“œ
            f = io.BytesIO(response.content)
            reader = PdfReader(f)
            
            total_pages = len(reader.pages)
            print(f"ğŸ“– íŒŒì‹± ì‹œì‘ (ì´ {total_pages} í˜ì´ì§€)")
            
            extracted_docs = []
            
            for i, page in enumerate(reader.pages):
                raw_text = page.extract_text()
                cleaned_text = self._clean_text(raw_text)
                
                # ë„ˆë¬´ ì§§ì€ í˜ì´ì§€(ëª©ì°¨, ë¹ˆ í˜ì´ì§€) ìŠ¤í‚µ
                if len(cleaned_text) < 50:
                    continue
                
                # í˜ì´ì§€ë³„ë¡œ ë¬¸ì„œ ìƒì„±
                chunks = self._extract_articles(cleaned_text, i + 1, doc_key)
                
                for chunk in chunks:
                    # F1NewsDocument ìŠ¤í‚¤ë§ˆì— ë§¤í•‘
                    doc = F1NewsDocument(
                        title=chunk['title'],
                        content=chunk['content'],
                        url=f"{target_url}#page={chunk['page_no']}", # PDF í˜ì´ì§€ ì•µì»¤ ì¶”ê°€
                        platform="FIA Official PDF",
                        author="FIA",
                        published_at=datetime.now(),
                        is_embedded=False
                    )
                    extracted_docs.append(doc)
            
            print(f" ë³€í™˜ ì™„ë£Œ: {len(extracted_docs)}ê°œì˜ ì²­í¬ í™•ë³´")
            return extracted_docs

        except Exception as e:
            print(f" FIA í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            print(traceback.format_exc())
            return []

# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    crawler = FIARegulationCrawler()
    # 2026 í…Œí¬ë‹ˆì»¬ ê·œì • í…ŒìŠ¤íŠ¸
    docs = crawler.crawl("2026_technical")
    
    if docs:
        print("\n--- Sample Document ---")
        print(f"Title: {docs[10].title}") # 10ë²ˆì§¸ í˜ì´ì§€(ë³´í†µ ë³¸ë¬¸ ì‹œì‘) í™•ì¸
        print(f"URL: {docs[10].url}")
        print(f"Content Preview: {docs[10].content[:200]}...")