import io
import re
import requests
import traceback
import os
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from beanie.operators import Set
from datetime import datetime
from pypdf import PdfReader
from domain.documents import F1NewsDocument

class FIARegulationCrawler:
    def __init__(self):
        # 2026ë…„ ê·œì • (Issue 8, Issue 2 ë“± ìµœì‹  ë²„ì „ ë°˜ì˜)
        self.urls = {
            "2026_technical": "https://www.fia.com/sites/default/files/fia_2026_formula_1_technical_regulations_issue_8_-_2024-06-24.pdf",
            "2026_sporting": "https://api.fia.com/sites/default/files/fia_2026_f1_regulations_-_section_b_sporting_-_iss02_-_2024-12-11.pdf",
            # í•„ìš”í•œ ê²½ìš° 2025ë…„ë„ ì¶”ê°€ ê°€ëŠ¥
            "2025_sporting": "https://www.fia.com/sites/default/files/fia_2025_formula_1_sporting_regulations_-_issue_1_-_2024-07-31.pdf",
            "2025_technical": "https://www.fia.com/sites/default/files/fia_2025_formula_1_technical_regulations_-_issue_01_-_2024-12-11_1.pdf"
        }

    def _clean_text(self, text: str) -> str:
        """
        PDF ë…¸ì´ì¦ˆ ì œê±°: 
        1. í˜ì´ì§€ ë²ˆí˜¸ (Page X of Y)
        2. ìƒë‹¨ í—¤ë” (Issue X, Date ë“±)
        3. ë¶ˆí•„ìš”í•œ ê³µë°±
        """
        if not text: return ""

        # 1. í—¤ë”/í‘¸í„° íŒ¨í„´ ì œê±° (ì˜ˆ: "Issue 8", "24 June 2024", "Page 1/100")
        # (ë‹¨ìˆœí•œ íŒ¨í„´ ë§¤ì¹­ì´ë¯€ë¡œ ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì„)
        text = re.sub(r'Page \d+/\d+', '', text) 
        text = re.sub(r'\d{1,2} [A-Za-z]+ \d{4}', '', text) # ë‚ ì§œ í˜•ì‹ ì œê±°
        text = re.sub(r'Issue \d+', '', text)

        # 2. ë‹¤ì¤‘ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def _extract_articles(self, text: str, page_num: int, doc_type: str) -> list:
        """
        (Advanced) í˜ì´ì§€ ë‚´ì—ì„œ 'Article X.Y' íŒ¨í„´ì„ ì°¾ì•„ì„œ 
        ë‹¨ìˆœ í˜ì´ì§€ ë²ˆí˜¸ë³´ë‹¤ ë” ì˜ë¯¸ ìˆëŠ” ì œëª©ì„ ìƒì„±í•˜ë ¤ ë…¸ë ¥í•¨.
        """
        # ì •ê·œì‹ìœ¼ë¡œ 'ARTICLE 1.2' ê°™ì€ íŒ¨í„´ ì°¾ê¸°
        article_matches = re.findall(r'ARTICLE\s+(\d+(\.\d+)*)', text, re.IGNORECASE)
        
        chunks = []
        
        # ë§Œì•½ í˜ì´ì§€ ì•ˆì— ëª…í™•í•œ Article ì‹œì‘ì ì´ ìˆë‹¤ë©´ ì œëª©ì— í¬í•¨
        if article_matches:
            # ê°€ì¥ ì²˜ìŒ ë°œê²¬ëœ Article ë²ˆí˜¸ë¥¼ ëŒ€í‘œ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
            main_article = article_matches[0][0]
            title = f"FIA {doc_type} Regulations - Art {main_article} (Page {page_num})"
        else:
            title = f"FIA {doc_type} Regulations - Page {page_num}"

        # ë°ì´í„° ìŠ¤í‚¤ë§ˆ ë§ì¶”ê¸° (F1NewsDocument ì¬í™œìš©)
        # ê·œì •ì§‘ì€ 'ë‰´ìŠ¤'ëŠ” ì•„ë‹ˆì§€ë§Œ, RAGì—ì„œëŠ” ê°™ì€ ë¬¸ì„œë¡œ ì·¨ê¸‰í•˜ëŠ” ê²Œ ê´€ë¦¬í•˜ê¸° í¸í•¨
        chunks.append({
            "title": title,
            "content": text,
            "page_no": page_num
        })
        
        return chunks

    def crawl(self, doc_key: str) -> list:
        """
        ì‹¤ì œ í¬ë¡¤ë§ ë° íŒŒì‹± ì‹¤í–‰
        doc_key: '2026_technical', '2026_sporting' ë“±
        """
        target_url = self.urls.get(doc_key)
        if not target_url:
            print(f" ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ í‚¤ì…ë‹ˆë‹¤: {doc_key}")
            return []

        print(f"ğŸ“¥ FIA ê·œì •ì§‘ ë‹¤ìš´ë¡œë“œ ì¤‘... [{doc_key}]")
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(target_url, headers=headers, timeout=60)
            
            if response.status_code != 200:
                print(f" ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: Status {response.status_code}")
                return []
            
            # PDF ë©”ëª¨ë¦¬ ë¡œë“œ
            f = io.BytesIO(response.content)
            reader = PdfReader(f)
            
            total_pages = len(reader.pages)
            print(f"ğŸ“– íŒŒì‹± ì‹œì‘ (ì´ {total_pages} í˜ì´ì§€)")
            
            extracted_docs = []
            
            for i, page in enumerate(reader.pages):
                raw_text = page.extract_text()
                cleaned_text = self._clean_text(raw_text)
                
                # ë„ˆë¬´ ì§§ì€ í˜ì´ì§€(ëª©ì°¨, ë¹ˆ í˜ì´ì§€) ìŠ¤í‚µ
                if len(cleaned_text) < 50:
                    continue
                
                # í˜ì´ì§€ë³„ë¡œ ë¬¸ì„œ ìƒì„±
                chunks = self._extract_articles(cleaned_text, i + 1, doc_key)
                
                for chunk in chunks:
                    # F1NewsDocument ìŠ¤í‚¤ë§ˆì— ë§¤í•‘
                    doc = F1NewsDocument(
                        title=chunk['title'],
                        content=chunk['content'],
                        url=f"{target_url}#page={chunk['page_no']}", # PDF í˜ì´ì§€ ì•µì»¤ ì¶”ê°€
                        platform="FIA Official PDF",
                        author="FIA",
                        published_at=datetime.now(),
                        is_embedded=False
                    )
                    extracted_docs.append(doc)
            
            print(f" ë³€í™˜ ì™„ë£Œ: {len(extracted_docs)}ê°œì˜ ì²­í¬ í™•ë³´")
            return extracted_docs

        except Exception as e:
            print(f" FIA í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            print(traceback.format_exc())
            return []

# ... (ìœ„ìª½ import ë° í´ë˜ìŠ¤ ì •ì˜ëŠ” ê·¸ëŒ€ë¡œ) ...
## ì ì¬
if __name__ == "__main__":
    import asyncio
    from motor.motor_asyncio import AsyncIOMotorClient
    from beanie import init_beanie
    
    # ğŸ“Œ ë¡œì»¬ ì‹¤í–‰ìš© ë©”ì¸ í•¨ìˆ˜
    async def main():
        print(" [Manual Run] MongoDB ì—°ê²° ì¤‘...")
        
        # 1. DB ì—°ê²° (ë¡œì»¬ í™˜ê²½ì— ë§ê²Œ ì£¼ì†Œ ì¡°ì •)
        # docker-composeë¡œ ë„ìš´ ëª½ê³ DBê°€ 27017í¬íŠ¸ë¡œ ì—´ë ¤ìˆë‹¤ê³  ê°€ì •
        client = AsyncIOMotorClient("mongodb://localhost:27017")
        
        # 2. Beanie ì´ˆê¸°í™”
        await init_beanie(database=client.pitwall_db, document_models=[F1NewsDocument])
        
        crawler = FIARegulationCrawler()
        
        # ìˆ˜ì§‘í•  íƒ€ê²Ÿ ì •ì˜ (2026 í…Œí¬ë‹ˆì»¬ & ìŠ¤í¬íŒ…)
        targets = ["2026_technical", "2026_sporting"]
        
        print("\n" + "="*50)
        print(" FIA ê·œì •ì§‘ ìˆ˜ë™ ì—…ë°ì´íŠ¸ ì‹œì‘")
        print("="*50)

        total_saved = 0

        for doc_type in targets:
            print(f"\n ìˆ˜ì§‘ ì‹œì‘: {doc_type}")
            docs = crawler.crawl(doc_type)
            
            if not docs:
                print(" ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            print(f" DB ì €ì¥ ì¤‘... ({len(docs)} pages)")
            saved_count = 0
            updated_count = 0
            
            for doc in docs:
                #  [Upsert Logic] 
                # 1. ê¸°ì¡´ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸ (ì¹´ìš´íŒ… ëª©ì )
                existing_doc = await F1NewsDocument.find_one(F1NewsDocument.url == doc.url)
                
                # 2. Upsert ìˆ˜í–‰ (í•µì‹¬!)
                # URLì´ ê°™ìœ¼ë©´ -> Set ë‚´ë¶€ì˜ í•„ë“œë§Œ ê°±ì‹ 
                # URLì´ ë‹¤ë¥´ë©´ -> on_insertì— ìˆëŠ” doc ì „ì²´ë¥¼ ì €ì¥
                await F1NewsDocument.find_one(F1NewsDocument.url == doc.url).upsert(
                    Set({
                        F1NewsDocument.title: doc.title,
                        F1NewsDocument.content: doc.content,
                        F1NewsDocument.published_at: datetime.now(), # ê°±ì‹  ì‹œê° ì—…ë°ì´íŠ¸
                        F1NewsDocument.is_embedded: False # ğŸŒŸ ì¤‘ìš”: ë‚´ìš©ì´ ë°”ë€Œì—ˆìœ¼ë‹ˆ ë‹¤ì‹œ ì„ë² ë”© ëŒ€ìƒì´ ë¨
                    }),
                    on_insert=doc
                )
                
                if existing_doc:
                    updated_count += 1
                else:
                    saved_count += 1
            
            print(f" {doc_type} ì™„ë£Œ: ì‹ ê·œ {saved_count}ê±´ / ê°±ì‹  {updated_count}ê±´")
            total_saved += saved_count

        print("\n" + "="*50)
        print(f" ì‘ì—… ì¢…ë£Œ. ì´ {total_saved} í˜ì´ì§€ê°€ DBì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        

    # ì‹¤í–‰
    asyncio.run(main())