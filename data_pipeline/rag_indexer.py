## MongoDBì—ì„œ ì›ë³¸ ë°ì´í„°ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ,,,

### [ì •ì œ -> ì²­í‚¹ -> ì„ë² ë”© -> ë²¡í„°DB ì ì¬ ë¡œì§] ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
### ì°¨í›„ Airflow DAGì—ì„œ PythonOperatorë¡œ í˜¸ì¶œimport re

import re
import asyncio
from typing import List
from motor.motor_asyncio import AsyncIOMotorClient
from beanie import init_beanie

# [ë„êµ¬ë“¤]
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance

# [ë„ë©”ì¸]
from domain.documents import F1NewsDocument


class RAGIndexer:
    def __init__(self, mongo_uri: str, qdrant_url: str):
        # 1. MongoDB ì—°ê²° ì¤€ë¹„
        self.mongo_uri = mongo_uri
        
        # 2. Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        self.qdrant = QdrantClient(url=qdrant_url)
        self.collection_name = "f1_knowledge_base"
        
        # 3. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (BAAI/bge-m3)
        # (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)
        print(" ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘ (BAAI/bge-m3)...")
        self.model = SentenceTransformer('BAAI/bge-m3')
        self.vector_size = 1024 # bge-m3ì˜ ë²¡í„° ì°¨ì› ìˆ˜
        
        # 4. Qdrant ì»¬ë ‰ì…˜ ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±)
        self._init_qdrant_collection()

    def _init_qdrant_collection(self):
        """Qdrantì— ë²¡í„° ì €ì¥ì†Œ ê³µê°„(Collection)ì„ ë§Œë“­ë‹ˆë‹¤."""
        if not self.qdrant.collection_exists(self.collection_name):
            self.qdrant.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE),
            )
            print(f" Qdrant ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name}")

    def clean_text(self, text: str) -> str:
        """[Step 1] í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text: return ""
        # 1. ê³¼ë„í•œ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'\n+', '\n', text) 
        text = re.sub(r'\s+', ' ', text)
        # 2. "Related Articles" ê°™ì€ ë…¸ì´ì¦ˆ ì œê±° (í•„ìš”ì‹œ íŒ¨í„´ ì¶”ê°€)
        text = text.replace("Load more", "").replace("Subscribe", "")
        return text.strip()

    def chunk_text(self, text: str) -> List[str]:
        """[Step 2] í…ìŠ¤íŠ¸ ì²­í‚¹ (LangChain ë¡œì§)"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,       # í•œ ë©ì–´ë¦¬ í¬ê¸° (ì)
            chunk_overlap=100,    # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ê²¹ì¹˜ëŠ” êµ¬ê°„
            separators=["\n\n", "\n", ".", " ", ""] # ìë¥´ëŠ” ìš°ì„ ìˆœìœ„
        )
        return splitter.split_text(text)

    def embed_text(self, chunks: List[str]) -> List[List[float]]:
        """[Step 3] ì„ë² ë”© (Text -> Vector)"""
        # sentence-transformersëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•´ì¤ë‹ˆë‹¤ (Batch)
        embeddings = self.model.encode(chunks, normalize_embeddings=True)
        return embeddings.tolist()

    async def run_indexing(self):
        """[Step 4] ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ (MongoDB -> Qdrant)"""
        print("ğŸš€ ì¸ë±ì‹± ì‘ì—… ì‹œì‘...")
        
        # 1. DB ì—°ê²°
        client = AsyncIOMotorClient(self.mongo_uri)
        await init_beanie(database=client.pitwall_db, document_models=[F1NewsDocument])
        
        # 2. ì•„ì§ ë²¡í„°í™”ë˜ì§€ ì•Šì€ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        # (ì§€ê¸ˆì€ í…ŒìŠ¤íŠ¸ë¼ 'ëª¨ë“ ' ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë‚˜ì¤‘ì—” flag í•„í„°ë§ í•„ìš”)
        docs = await F1NewsDocument.find_all().to_list()
        print(f"ğŸ“¦ MongoDBì—ì„œ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        total_chunks = 0
        
        for doc in docs:
            # A. ì •ì œ
            cleaned_content = self.clean_text(doc.content)
            if len(cleaned_content) < 50: continue # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ

            # B. ì²­í‚¹
            chunks = self.chunk_text(cleaned_content)
            if not chunks: continue

            # C. ì„ë² ë”©
            vectors = self.embed_text(chunks)

            # D. Qdrant ì—…ë¡œë“œ (Batch Upload)
            points = []
            for i, (chunk, vector) in enumerate(zip(chunks, vectors)):
                # ID ìƒì„±: ë¬¸ì„œID_ì²­í¬ìˆœë²ˆ
                point_id = f"{doc.id}_{i}"
                
                # ë©”íƒ€ë°ì´í„°: ì¶œì²˜ í™•ì¸ì„ ìœ„í•´ ì¤‘ìš”!
                payload = {
                    "source_url": doc.url,
                    "title": doc.title,
                    "platform": doc.platform,
                    "published_at": doc.published_at.isoformat() if doc.published_at else None,
                    "text": chunk  # ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ì—¬ì¤„ ì›ë³¸ í…ìŠ¤íŠ¸
                }
                
                # QdrantëŠ” UUID í¬ë§·ì˜ IDë¥¼ ì„ í˜¸í•˜ì§€ë§Œ, ë¬¸ìì—´ í•´ì‹œë¥¼ ì¨ë„ ë¨.
                # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ UUID ìƒì„±ì„ ìœ„í•´ qdrantê°€ ì œê³µí•˜ëŠ” ìœ í‹¸ë¦¬í‹° ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜
                # ê°„ë‹¨íˆ UUID íŒ¨í‚¤ì§€ ì‚¬ìš©í•´ì„œ ê³ ìœ  ID ìƒì„± ì¶”ì²œ. 
                import uuid
                # ê³ ìœ  ID ìƒì„± (Deterministicí•˜ê²Œ ë§Œë“¤ë©´ ì¤‘ë³µ ë°©ì§€ì— ì¢‹ìŒ)
                point_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, point_id))

                points.append(PointStruct(id=point_uuid, vector=vector, payload=payload))

            # Qdrantì— ì €ì¥
            self.qdrant.upsert(
                collection_name=self.collection_name,
                points=points
            )
            total_chunks += len(chunks)
            print(f" -> ë¬¸ì„œ '{doc.title[:20]}...' ì²˜ë¦¬ ì™„ë£Œ ({len(chunks)} Chunks)")

        print(f" ì¸ë±ì‹± ì™„ë£Œ! ì´ {total_chunks}ê°œì˜ ì²­í¬ê°€ Qdrantì— ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©) ---
if __name__ == "__main__":
    # ë¡œì»¬ ì„¤ì •
    indexer = RAGIndexer(
        mongo_uri="mongodb://admin:password123@localhost:27017",
        qdrant_url="http://localhost:6333"
    )
    asyncio.run(indexer.run_indexing())