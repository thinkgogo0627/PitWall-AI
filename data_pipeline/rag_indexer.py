import os
import uuid
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from typing import List
from motor.motor_asyncio import AsyncIOMotorClient
from qdrant_client import QdrantClient
from qdrant_client.http import models
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding

# ë„ë©”ì¸ ëª¨ë¸ (Beanie Document)
from domain.documents import F1NewsDocument

class RAGIndexer:
    def __init__(self, mongo_uri: str, qdrant_url: str, qdrant_api_key=None):
        self.mongo_uri = mongo_uri
        self.qdrant_url = qdrant_url
        self.qdrant_api_key = qdrant_api_key
        self.collection_name = "f1_knowledge_base"
        
        # 1. Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        print(f"ğŸ”Œ [Indexer] Connecting to Qdrant: {self.qdrant_url}")
        self.qdrant_client = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_api_key)
        
        # [ìˆ˜ì •] ì„ë² ë”© ëª¨ë¸ êµì²´ (API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        api_key = os.getenv("GOOGLE_API_KEY")
        
        print(f"ğŸ”Œ [Indexer] Loading Google Gemini Embedding Model...")
        self.embed_model = GoogleGenAIEmbedding(
            model_name="models/gemini-embedding-001",  # ìµœì‹  ëª¨ë¸ (ì„±ëŠ¥ ì¢‹ìŒ)
            api_key=api_key
        )
        

    def _generate_deterministic_uuid(self, text: str) -> str:
        """URL ê¸°ë°˜ìœ¼ë¡œ í•­ìƒ ê°™ì€ UUIDë¥¼ ìƒì„± (ë©±ë“±ì„± ë³´ì¥ í•µì‹¬)"""
        return str(uuid.uuid5(uuid.NAMESPACE_URL, text))

    async def run_indexing(self, batch_size: int = 50):
        """MongoDBì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ Qdrantì— ì ì¬"""
        print(" Indexing Started...")
        
        # 1. MongoDB ì—°ê²°
        client = AsyncIOMotorClient(self.mongo_uri)
        db = client.pitwall_db
        # Beanie ì´ˆê¸°í™”ê°€ ì•ˆ ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ raw query ì‚¬ìš©
        # (Beanie ì˜ì¡´ì„±ì„ ì¤„ì—¬ì„œ ê°€ë³ê²Œ ì‹¤í–‰)
        collection = db.get_collection("f1_news_articles")
        
        # 2. Qdrant ì»¬ë ‰ì…˜ ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±) & ì¸ë±ìŠ¤ ì„¤ì •
        if not self.qdrant_client.collection_exists(self.collection_name):
            print(f" Creating Collection: {self.collection_name}")
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=3072, # Gemini embedding
                    distance=models.Distance.COSINE
                )
            )
            # [ìµœì í™”] í•„í„°ë§ ìì£¼ í•˜ëŠ” í•„ë“œ ì¸ë±ì‹±
            self.qdrant_client.create_payload_index(
                collection_name=self.collection_name,
                field_name="platform",
                field_schema=models.PayloadSchemaType.KEYWORD
            )
            self.qdrant_client.create_payload_index(
                collection_name=self.collection_name,
                field_name="published_at",
                field_schema=models.PayloadSchemaType.DATETIME
            )

        # 3. ë°ì´í„° íŒ¨ì¹˜ ë° ì„ë² ë”© (Batch Processing)
        # ì•„ì§ ë²¡í„°í™”ë˜ì§€ ì•Šì€(í˜¹ì€ ì „ì²´) ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ 'ì „ì²´ ìŠ¤ìº” í›„ Upsert' ë°©ì‹ìœ¼ë¡œ ë©±ë“±ì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        cursor = collection.find({}) 
        
        batch_docs = []
        total_indexed = 0
        
        async for doc in cursor:
            batch_docs.append(doc)
            
            if len(batch_docs) >= batch_size:
                await self._process_batch(batch_docs)
                total_indexed += len(batch_docs)
                batch_docs = [] # ì´ˆê¸°í™”

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch_docs:
            await self._process_batch(batch_docs)
            total_indexed += len(batch_docs)
            
        print(f" Indexing Finished! Total {total_indexed} documents processed.")

    async def _process_batch(self, docs: List[dict]):
        """ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”© ë° ì—…ë¡œë“œ"""
        texts = [d.get('content', '')[:8000] for d in docs]
        metadatas = []
        ids = []
        
        for d in docs:
            url = d.get('url', '')
            ids.append(self._generate_deterministic_uuid(url))
            
            metadatas.append({
                "title": d.get('title', ''),
                "url": url,
                "platform": d.get('platform', 'Unknown'),
                "published_at": d.get('published_at', '').isoformat() if d.get('published_at') else None,
                "text": d.get('content', '')[:1000]
            })

        # 1. [ìˆ˜ì •] êµ¬ê¸€ APIë¡œ í•œ ë°©ì— ì„ë² ë”© (Batch API)
        if not texts: return
        
        # encode() -> get_text_embedding_batch() ë¡œ ë³€ê²½
        # LlamaIndexëŠ” ê¸°ë³¸ì ìœ¼ë¡œ List[float]ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ numpy ë³€í™˜ ì˜µì…˜ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        embeddings = self.embed_model.get_text_embedding_batch(texts)
        
        # 2. Qdrant Points ìƒì„±
        points = [
            models.PointStruct(
                id=id_,
                # LlamaIndex ê²°ê³¼ëŠ” ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ .tolist()ë¥¼ êµ³ì´ í•  í•„ìš” ì—†ì§€ë§Œ, 
                # ì•ˆì „í•˜ê²Œ ê°€ë ¤ë©´ ê·¸ëƒ¥ ë‘¬ë„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì—” .tolist()ê°€ ì—†ì–´ì„œ ì—ëŸ¬ë‚  ìˆ˜ ìˆìŒ.
                # ê·¸ëƒ¥ embedding ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.
                vector=embedding, 
                payload=metadata
            )
            for id_, embedding, metadata in zip(ids, embeddings, metadatas)
        ]
        
        # 3. ì—…ë¡œë“œ
        self.qdrant_client.upsert(
            collection_name=self.collection_name,
            points=points
        )
        print(f" Batch Upserted: {len(points)} items")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš©
if __name__ == "__main__":
    import asyncio
    indexer = RAGIndexer(
        mongo_uri="mongodb://localhost:27017", # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œ
        qdrant_url="http://localhost:6333"
    )
    asyncio.run(indexer.run_indexing())