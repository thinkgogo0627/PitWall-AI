import os
import uuid
import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from typing import List
from motor.motor_asyncio import AsyncIOMotorClient
from qdrant_client import QdrantClient
from qdrant_client.http import models
from sentence_transformers import SentenceTransformer
import torch # GPU ì²´í¬ìš©

# ë„ë©”ì¸ ëª¨ë¸ (Beanie Document)
from domain.documents import F1NewsDocument

class RAGIndexer:
    def __init__(self, mongo_uri: str, qdrant_url: str):
        self.mongo_uri = mongo_uri
        self.qdrant_url = qdrant_url
        self.collection_name = "f1_knowledge_base"
        
        # 1. Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        print(f"ğŸ”Œ [Indexer] Connecting to Qdrant: {self.qdrant_url}")
        self.qdrant_client = QdrantClient(url=self.qdrant_url)
        
        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (GPU ê°€ì† í™•ì¸)
        # ë¡œì»¬ ìºì‹œ ê²½ë¡œ ìš°ì„  í™•ì¸ (ë¡œì»¬ì— ì„¤ì •í•œ ê·¸ ê²½ë¡œ!)
        docker_model_path = "/app/models/bge-m3"
        local_model_path = os.path.join(os.path.dirname(__file__), "../data/model_cache/bge-m3")
        
        model_path = "BAAI/bge-m3" # ê¸°ë³¸ê°’
        if os.path.exists(docker_model_path):
            print(f"ğŸ“‚ Loading from docker Path: {docker_model_path}")
            model_path = docker_model_path
        elif os.path.exists(local_model_path):
            print(f"ğŸ’» Loading from Local Path: {local_model_path}")
            model_path = local_model_path
        else:
            print("ğŸŒ Model not found locally. Downloading from HuggingFace Hub...")

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f" [Indexer] Loading Model on [{device.upper()}] from {model_path}...")
        
        self.embed_model = SentenceTransformer(model_path, device=device)

    def _generate_deterministic_uuid(self, text: str) -> str:
        """URL ê¸°ë°˜ìœ¼ë¡œ í•­ìƒ ê°™ì€ UUIDë¥¼ ìƒì„± (ë©±ë“±ì„± ë³´ì¥ í•µì‹¬)"""
        return str(uuid.uuid5(uuid.NAMESPACE_URL, text))

    async def run_indexing(self, batch_size: int = 50):
        """MongoDBì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ Qdrantì— ì ì¬"""
        print(" Indexing Started...")
        
        # 1. MongoDB ì—°ê²°
        client = AsyncIOMotorClient(self.mongo_uri)
        db = client.pitwall_db
        # Beanie ì´ˆê¸°í™”ê°€ ì•ˆ ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ raw query ì‚¬ìš©
        # (Beanie ì˜ì¡´ì„±ì„ ì¤„ì—¬ì„œ ê°€ë³ê²Œ ì‹¤í–‰)
        collection = db.get_collection("f1_news_articles")
        
        # 2. Qdrant ì»¬ë ‰ì…˜ ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±) & ì¸ë±ìŠ¤ ì„¤ì •
        if not self.qdrant_client.collection_exists(self.collection_name):
            print(f" Creating Collection: {self.collection_name}")
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=models.VectorParams(
                    size=1024, # bge-m3 output dimension
                    distance=models.Distance.COSINE
                )
            )
            # [ìµœì í™”] í•„í„°ë§ ìì£¼ í•˜ëŠ” í•„ë“œ ì¸ë±ì‹±
            self.qdrant_client.create_payload_index(
                collection_name=self.collection_name,
                field_name="platform",
                field_schema=models.PayloadSchemaType.KEYWORD
            )
            self.qdrant_client.create_payload_index(
                collection_name=self.collection_name,
                field_name="published_at",
                field_schema=models.PayloadSchemaType.DATETIME
            )

        # 3. ë°ì´í„° íŒ¨ì¹˜ ë° ì„ë² ë”© (Batch Processing)
        # ì•„ì§ ë²¡í„°í™”ë˜ì§€ ì•Šì€(í˜¹ì€ ì „ì²´) ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ 'ì „ì²´ ìŠ¤ìº” í›„ Upsert' ë°©ì‹ìœ¼ë¡œ ë©±ë“±ì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        cursor = collection.find({}) 
        
        batch_docs = []
        total_indexed = 0
        
        async for doc in cursor:
            batch_docs.append(doc)
            
            if len(batch_docs) >= batch_size:
                await self._process_batch(batch_docs)
                total_indexed += len(batch_docs)
                batch_docs = [] # ì´ˆê¸°í™”

        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch_docs:
            await self._process_batch(batch_docs)
            total_indexed += len(batch_docs)
            
        print(f" Indexing Finished! Total {total_indexed} documents processed.")

    async def _process_batch(self, docs: List[dict]):
        """ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”© ë° ì—…ë¡œë“œ"""
        texts = [d.get('content', '')[:8000] for d in docs] # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì˜ë¼ë‚´ê¸°
        metadatas = []
        ids = []
        
        for d in docs:
            url = d.get('url', '')
            # [ë©±ë“±ì„± í•µì‹¬] URLë¡œ ID ìƒì„±
            ids.append(self._generate_deterministic_uuid(url))
            
            metadatas.append({
                "title": d.get('title', ''),
                "url": url,
                "platform": d.get('platform', 'Unknown'),
                "published_at": d.get('published_at', '').isoformat() if d.get('published_at') else None,
                "text": d.get('content', '')[:1000] # Qdrant Payloadì— ì €ì¥í•  ë³¸ë¬¸ (ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œìš©)
            })

        # 1. GPUë¡œ í•œ ë°©ì— ì„ë² ë”©
        if not texts: return
        embeddings = self.embed_model.encode(texts, convert_to_numpy=True)
        
        # 2. Qdrant Points ìƒì„±
        points = [
            models.PointStruct(
                id=id_,
                vector=embedding.tolist(),
                payload=metadata
            )
            for id_, embedding, metadata in zip(ids, embeddings, metadatas)
        ]
        
        # 3. ì—…ë¡œë“œ (Upsert: ê¸°ì¡´ ID ìˆìœ¼ë©´ ë®ì–´ì”€)
        self.qdrant_client.upsert(
            collection_name=self.collection_name,
            points=points
        )
        print(f" Batch Upserted: {len(points)} items")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš©
if __name__ == "__main__":
    import asyncio
    indexer = RAGIndexer(
        mongo_uri="mongodb://localhost:27017", # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œ
        qdrant_url="http://localhost:6333"
    )
    asyncio.run(indexer.run_indexing())